{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d2b113",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ef2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the main of the project\n",
    "\n",
    "# libraries\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import os\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "import statistics as st\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score \n",
    "from sklearn.cluster import KMeans \n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import *\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# custom libraries\n",
    "from Geographycal_functions import drop_non_geolocalised\n",
    "from Geographycal_functions import localize_tweets\n",
    "from Geographycal_functions import localize_USA\n",
    "from Preprocessing_functions import parallelize_dataframe\n",
    "from Preprocessing_functions import text_preprocessing\n",
    "from Preprocessing_functions import text_mining\n",
    "from Preprocessing_functions import safe_drop_attr\n",
    "from Preprocessing_functions import select_dates_tweets\n",
    "\n",
    "\n",
    "# visualize progresses\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3593a",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d9366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\".\\Processed_data\\States_mineddf_total.csv\"):                       # we can choose one of the two\n",
    "    state_df = pd.read_csv(\".\\Processed_data\\States_mined.csv\")\n",
    "    # finish here\n",
    "    undone_flag = False\n",
    "else:\n",
    "    # import the raw data\n",
    "    data_donald = pd.read_csv(\"hashtag_donaldtrump.csv\", lineterminator='\\n')\n",
    "    data_joe = pd.read_csv(\"hashtag_joebiden.csv\", lineterminator='\\n')\n",
    "    \n",
    "    # joining the two datasets dropping duplicates!!\n",
    "    data_all = pd.concat([data_joe,data_donald]).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # flag to be used in the next steps of the project\n",
    "    undone_flag = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6cc2b",
   "metadata": {},
   "source": [
    "# Preprocessing - Geolocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a085e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    print(\"Geolocalization\")\n",
    "    data_all = drop_non_geolocalised(data_all, \"lat\", \"long\")\n",
    "    geo_df = localize_tweets(data_all, \"World Tweet data distribution\")\n",
    "    df = localize_USA(geo_df, \"USA Tweets data distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389ec99",
   "metadata": {},
   "source": [
    "# Preprocessing - text mining\n",
    "We considered only the data localized in the USA.\n",
    "\n",
    "This diminished the number of instances by a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea130fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    # data to drop\n",
    "    to_be_deleted = [\"tweet_id\", \"source\", \"user_id\", \"user_join_date\", \"user_location\", \"continent\", \"collected_at\"]\n",
    "    # drop\n",
    "    df = safe_drop_attr(df, to_be_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce09ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    # Text mining\n",
    "    df = text_preprocessing(df)\n",
    "    \n",
    "    # save those preliminary results, since run_time is high\n",
    "    df.to_csv(\".\\Processed_data\\Df_languages_detected.csv\")\n",
    "    \n",
    "    df, lang_state_df = text_mining(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb4fcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    # save the data into a folder\n",
    "    df.to_csv(\".\\Processed_data\\Df_mined.csv\")\n",
    "    lang_state_df.to_csv(\".\\Processed_data\\lang_share_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6595b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    # get the lenght of the dataframe to normalize the data\n",
    "    num_instances = len(df[\"STATE_NAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8205a",
   "metadata": {},
   "source": [
    "# Select different time-frames from our data\n",
    "\n",
    "In particular we select the dates before the last public debate, before the elections and after the election day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155d693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(388853, 21)\n",
      "Number of tweets till the day of the last debate:  88636\n",
      "Number of tweets till the day of the elections:  263306\n",
      "Number of total tweets:  388853\n"
     ]
    }
   ],
   "source": [
    "undone_flag = True\n",
    "df = pd.read_csv(\".\\Processed_data\\Df_mined.csv\")\n",
    "\n",
    "#lang_state_df = pd.read_csv(\".\\Processed_data\\States_mined.csv\", usecols=[\"STATE_NAME\", \"%_english\"])\n",
    "print(df.shape)\n",
    "\n",
    "# get the temporal division!\n",
    "df_last_debate, df_election_day, df_total = select_dates_tweets(df)\n",
    "print(\"Number of tweets till the day of the last debate: \", len(df_last_debate))\n",
    "print(\"Number of tweets till the day of the elections: \", len(df_election_day))\n",
    "print(\"Number of total tweets: \", len(df_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5586e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dfs = [df_last_debate, df_election_day, df_total]\n",
    "supp_names = [\"df_last_debate\", \"df_election_day\", \"df_total\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2732c",
   "metadata": {},
   "source": [
    "# Create the ML dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bbfd6",
   "metadata": {},
   "source": [
    "Since we are squeezing all our data into 51 elements, we want to add to each state as much information about the statistical population they represents, we are therefore adding some statistics to the dataframe.\n",
    "Each statistic is related to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a78e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    for df in my_dfs:\n",
    "        df.drop(columns=[\"DRAWSEQ\", \"index_right\", \"Unnamed: 0\", \"Unnamed: 0.1\",\n",
    "                         \"STATE_FIPS\", \"SUB_REGION\", \"STATE_ABBR\", \"geometry\", \"created_at\",\n",
    "                        \"user_screen_name\", \"user_description\", \"clean_tweet\", \"Languages\", \"user_name\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e929479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beltr\\AppData\\Local\\Temp/ipykernel_12188/932968303.py:31: FutureWarning: Dropping invalid columns in DataFrameGroupBy.quantile is deprecated. In a future version, a TypeError will be raised. Before calling .quantile, select only columns which should be valid for the function.\n",
      "  state_list.append(df_groupby.quantile(q=0.1).add_suffix(\"_0_1_quant\"))\n",
      "C:\\Users\\beltr\\AppData\\Local\\Temp/ipykernel_12188/932968303.py:33: FutureWarning: Dropping invalid columns in DataFrameGroupBy.quantile is deprecated. In a future version, a TypeError will be raised. Before calling .quantile, select only columns which should be valid for the function.\n",
      "  state_list.append(df_groupby.quantile(q=0.9).add_suffix(\"_0_9_quant\"))\n",
      "C:\\Users\\beltr\\AppData\\Local\\Temp/ipykernel_12188/932968303.py:31: FutureWarning: Dropping invalid columns in DataFrameGroupBy.quantile is deprecated. In a future version, a TypeError will be raised. Before calling .quantile, select only columns which should be valid for the function.\n",
      "  state_list.append(df_groupby.quantile(q=0.1).add_suffix(\"_0_1_quant\"))\n",
      "C:\\Users\\beltr\\AppData\\Local\\Temp/ipykernel_12188/932968303.py:33: FutureWarning: Dropping invalid columns in DataFrameGroupBy.quantile is deprecated. In a future version, a TypeError will be raised. Before calling .quantile, select only columns which should be valid for the function.\n",
      "  state_list.append(df_groupby.quantile(q=0.9).add_suffix(\"_0_9_quant\"))\n",
      "C:\\Users\\beltr\\AppData\\Local\\Temp/ipykernel_12188/932968303.py:31: FutureWarning: Dropping invalid columns in DataFrameGroupBy.quantile is deprecated. In a future version, a TypeError will be raised. Before calling .quantile, select only columns which should be valid for the function.\n",
      "  state_list.append(df_groupby.quantile(q=0.1).add_suffix(\"_0_1_quant\"))\n",
      "C:\\Users\\beltr\\AppData\\Local\\Temp/ipykernel_12188/932968303.py:33: FutureWarning: Dropping invalid columns in DataFrameGroupBy.quantile is deprecated. In a future version, a TypeError will be raised. Before calling .quantile, select only columns which should be valid for the function.\n",
      "  state_list.append(df_groupby.quantile(q=0.9).add_suffix(\"_0_9_quant\"))\n"
     ]
    }
   ],
   "source": [
    "if undone_flag:\n",
    "    # creating dictionaries\n",
    "    state_df = {}\n",
    "    supp = 0\n",
    "    \n",
    "    # initialize the scaler\n",
    "    my_scaler = StandardScaler()\n",
    "    \n",
    "    for df in my_dfs:\n",
    "        # visualize the bar \n",
    "        tqdm.pandas(desc=\"Statistical mining: \")\n",
    "        num_instances = len(df)\n",
    "    \n",
    "        # initialize an empty list where to append the statistics\n",
    "        state_list = []\n",
    "        # group the data by state\n",
    "        df_groupby = df.groupby([\"STATE_NAME\"])\n",
    "        # averages\n",
    "        state_list.append(df_groupby.mean(numeric_only=True).add_suffix(\"_Mean\"))\n",
    "        # skewness\n",
    "        state_list.append(df_groupby.skew(numeric_only=True).add_suffix(\"_Skewness\"))  \n",
    "        # median\n",
    "        state_list.append(df_groupby.median(numeric_only=True).add_suffix(\"_Median\"))  \n",
    "        # count the tweets and normalize the count wrt the total number of instances\n",
    "        # state_list.append(df_groupby.count().add_suffix(\"_Counts\") / num_instances)  \n",
    "        # variance\n",
    "        state_list.append(df_groupby.var().add_suffix(\"_Variance\"))\n",
    "        # standard deviation\n",
    "        state_list.append(df_groupby.std().add_suffix(\"_Std\"))\n",
    "        # 0.1 quantile\n",
    "        state_list.append(df_groupby.quantile(q=0.1).add_suffix(\"_0_1_quant\"))\n",
    "        # 0.9 quantile\n",
    "        state_list.append(df_groupby.quantile(q=0.9).add_suffix(\"_0_9_quant\"))\n",
    "        \n",
    "        \n",
    "        # concatenate all those data to create a large dataframe\n",
    "        state_df[supp_names[supp]] = pd.concat(state_list, axis=1)\n",
    "        \n",
    "        # % of english speakers\n",
    "        # state_df[supp_names[supp]].merge(lang_state_df, on=\"STATE_NAME\")\n",
    "        \n",
    "        # create a key for the non categorical values\n",
    "        logic = [col != \"STATE_NAME\" for col in state_df[supp_names[supp]].columns]\n",
    "        original_key = state_df[supp_names[supp]].columns \n",
    "        key = state_df[supp_names[supp]].columns[logic]\n",
    "        \n",
    "        ## scale\n",
    "        #state_df[supp_names[supp] + \"_scaled\"] = my_scaler.fit_transform(\n",
    "        #    state_df[supp_names[supp]].loc[:, key])\n",
    "        #\n",
    "        ## trasform in a df\n",
    "        #state_df[supp_names[supp] + \"_scaled\"] = pd.DataFrame(\n",
    "        #    state_df[supp_names[supp] + \"_scaled\"], columns=key)\n",
    "        #\n",
    "        ## add the state name\n",
    "        #state_df[supp_names[supp] + \"_scaled\"] = pd.concat(\n",
    "        #    [pd.Series(df_groupby.groups.keys()), state_df[supp_names[supp] + \"_scaled\"]], axis=1)\n",
    "        \n",
    "        # save\n",
    "        state_df[supp_names[supp]].to_csv(\".\\Processed_data\\States_mined\" + supp_names[supp] + \".csv\")\n",
    "        \n",
    "        # update index\n",
    "        supp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31cd98a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes_Mean</th>\n",
       "      <th>retweet_count_Mean</th>\n",
       "      <th>user_followers_count_Mean</th>\n",
       "      <th>TextBlob_Subjectivity_Mean</th>\n",
       "      <th>TextBlob_Polarity_Mean</th>\n",
       "      <th>likes_Skewness</th>\n",
       "      <th>retweet_count_Skewness</th>\n",
       "      <th>user_followers_count_Skewness</th>\n",
       "      <th>TextBlob_Subjectivity_Skewness</th>\n",
       "      <th>TextBlob_Polarity_Skewness</th>\n",
       "      <th>...</th>\n",
       "      <th>likes_0_1_quant</th>\n",
       "      <th>retweet_count_0_1_quant</th>\n",
       "      <th>user_followers_count_0_1_quant</th>\n",
       "      <th>TextBlob_Subjectivity_0_1_quant</th>\n",
       "      <th>TextBlob_Polarity_0_1_quant</th>\n",
       "      <th>likes_0_9_quant</th>\n",
       "      <th>retweet_count_0_9_quant</th>\n",
       "      <th>user_followers_count_0_9_quant</th>\n",
       "      <th>TextBlob_Subjectivity_0_9_quant</th>\n",
       "      <th>TextBlob_Polarity_0_9_quant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATE_NAME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>3.875874</td>\n",
       "      <td>1.095280</td>\n",
       "      <td>8374.406469</td>\n",
       "      <td>0.339647</td>\n",
       "      <td>0.079804</td>\n",
       "      <td>14.649100</td>\n",
       "      <td>13.358431</td>\n",
       "      <td>3.177572</td>\n",
       "      <td>0.446737</td>\n",
       "      <td>0.430516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.157500</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8958.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>0.898662</td>\n",
       "      <td>0.271511</td>\n",
       "      <td>2464.818356</td>\n",
       "      <td>0.341844</td>\n",
       "      <td>0.051959</td>\n",
       "      <td>9.460180</td>\n",
       "      <td>21.212594</td>\n",
       "      <td>5.306806</td>\n",
       "      <td>0.332601</td>\n",
       "      <td>0.554995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.199062</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4055.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>2.654507</td>\n",
       "      <td>0.738056</td>\n",
       "      <td>2924.439398</td>\n",
       "      <td>0.319254</td>\n",
       "      <td>0.061104</td>\n",
       "      <td>15.211403</td>\n",
       "      <td>13.855179</td>\n",
       "      <td>12.340654</td>\n",
       "      <td>0.509926</td>\n",
       "      <td>0.096939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.207593</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7481.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.413333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>1.207643</td>\n",
       "      <td>0.449682</td>\n",
       "      <td>6192.387261</td>\n",
       "      <td>0.324843</td>\n",
       "      <td>0.060139</td>\n",
       "      <td>8.892309</td>\n",
       "      <td>7.217228</td>\n",
       "      <td>4.383891</td>\n",
       "      <td>0.428717</td>\n",
       "      <td>0.567680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.185227</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2673.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.341250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>4.302740</td>\n",
       "      <td>1.139293</td>\n",
       "      <td>6724.195873</td>\n",
       "      <td>0.320105</td>\n",
       "      <td>0.051099</td>\n",
       "      <td>106.402440</td>\n",
       "      <td>115.622329</td>\n",
       "      <td>33.325834</td>\n",
       "      <td>0.505455</td>\n",
       "      <td>0.206356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.225000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6004.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            likes_Mean  retweet_count_Mean  user_followers_count_Mean  \\\n",
       "STATE_NAME                                                              \n",
       "Alabama       3.875874            1.095280                8374.406469   \n",
       "Alaska        0.898662            0.271511                2464.818356   \n",
       "Arizona       2.654507            0.738056                2924.439398   \n",
       "Arkansas      1.207643            0.449682                6192.387261   \n",
       "California    4.302740            1.139293                6724.195873   \n",
       "\n",
       "            TextBlob_Subjectivity_Mean  TextBlob_Polarity_Mean  \\\n",
       "STATE_NAME                                                       \n",
       "Alabama                       0.339647                0.079804   \n",
       "Alaska                        0.341844                0.051959   \n",
       "Arizona                       0.319254                0.061104   \n",
       "Arkansas                      0.324843                0.060139   \n",
       "California                    0.320105                0.051099   \n",
       "\n",
       "            likes_Skewness  retweet_count_Skewness  \\\n",
       "STATE_NAME                                           \n",
       "Alabama          14.649100               13.358431   \n",
       "Alaska            9.460180               21.212594   \n",
       "Arizona          15.211403               13.855179   \n",
       "Arkansas          8.892309                7.217228   \n",
       "California      106.402440              115.622329   \n",
       "\n",
       "            user_followers_count_Skewness  TextBlob_Subjectivity_Skewness  \\\n",
       "STATE_NAME                                                                  \n",
       "Alabama                          3.177572                        0.446737   \n",
       "Alaska                           5.306806                        0.332601   \n",
       "Arizona                         12.340654                        0.509926   \n",
       "Arkansas                         4.383891                        0.428717   \n",
       "California                      33.325834                        0.505455   \n",
       "\n",
       "            TextBlob_Polarity_Skewness  ...  likes_0_1_quant  \\\n",
       "STATE_NAME                              ...                    \n",
       "Alabama                       0.430516  ...              0.0   \n",
       "Alaska                        0.554995  ...              0.0   \n",
       "Arizona                       0.096939  ...              0.0   \n",
       "Arkansas                      0.567680  ...              0.0   \n",
       "California                    0.206356  ...              0.0   \n",
       "\n",
       "            retweet_count_0_1_quant  user_followers_count_0_1_quant  \\\n",
       "STATE_NAME                                                            \n",
       "Alabama                         0.0                            23.0   \n",
       "Alaska                          0.0                            11.0   \n",
       "Arizona                         0.0                            32.0   \n",
       "Arkansas                        0.0                            41.8   \n",
       "California                      0.0                            25.0   \n",
       "\n",
       "            TextBlob_Subjectivity_0_1_quant  TextBlob_Polarity_0_1_quant  \\\n",
       "STATE_NAME                                                                 \n",
       "Alabama                                 0.0                    -0.157500   \n",
       "Alaska                                  0.0                    -0.199062   \n",
       "Arizona                                 0.0                    -0.207593   \n",
       "Arkansas                                0.0                    -0.185227   \n",
       "California                              0.0                    -0.225000   \n",
       "\n",
       "            likes_0_9_quant  retweet_count_0_9_quant  \\\n",
       "STATE_NAME                                             \n",
       "Alabama                 6.0                      2.0   \n",
       "Alaska                  2.0                      0.8   \n",
       "Arizona                 3.0                      1.0   \n",
       "Arkansas                2.0                      1.0   \n",
       "California              3.0                      1.0   \n",
       "\n",
       "            user_followers_count_0_9_quant  TextBlob_Subjectivity_0_9_quant  \\\n",
       "STATE_NAME                                                                    \n",
       "Alabama                             8958.0                             0.80   \n",
       "Alaska                              4055.0                             0.75   \n",
       "Arizona                             7481.0                             0.75   \n",
       "Arkansas                            2673.4                             0.70   \n",
       "California                          6004.0                             0.75   \n",
       "\n",
       "            TextBlob_Polarity_0_9_quant  \n",
       "STATE_NAME                               \n",
       "Alabama                        0.500000  \n",
       "Alaska                         0.400000  \n",
       "Arizona                        0.413333  \n",
       "Arkansas                       0.341250  \n",
       "California                     0.400000  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_df[\"df_election_day\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df[\"df_election_day\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c348d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df_scaled = my_scaler.fit_transform(state_df.loc[:, state_df.columns != 'STATE_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df_scaled = pd.DataFrame(state_df_scaled, columns=state_df.loc[:, state_df.columns != 'STATE_NAME'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df_scaled = pd.concat([state_df[\"STATE_NAME\"], state_df_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df_scaled.to_csv(\"State_df_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2dffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7967899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
