{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d2b113",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ef2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the main of the project\n",
    "\n",
    "# libraries\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import os\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "import statistics as st\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score \n",
    "from sklearn.cluster import KMeans \n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import *\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# custom libraries\n",
    "from Geographycal_functions import drop_non_geolocalised\n",
    "from Geographycal_functions import localize_tweets\n",
    "from Geographycal_functions import localize_USA\n",
    "from Preprocessing_functions import parallelize_dataframe\n",
    "from Preprocessing_functions import text_preprocessing\n",
    "from Preprocessing_functions import text_mining\n",
    "from Preprocessing_functions import safe_drop_attr\n",
    "\n",
    "\n",
    "\n",
    "# visualize progresses\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3593a",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d9366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\".\\Processed_data\\States_mined.csv\"):                       # we can choose one of the two\n",
    "    state_df = pd.read_csv(\".\\Processed_data\\States_mined.csv\")\n",
    "    undone_flag = False\n",
    "else:\n",
    "    # import the raw data\n",
    "    data_donald = pd.read_csv(\"hashtag_donaldtrump.csv\", lineterminator='\\n')\n",
    "    data_joe = pd.read_csv(\"hashtag_joebiden.csv\", lineterminator='\\n')\n",
    "    \n",
    "    # joining the two datasets dropping duplicates!!\n",
    "    data_all = pd.concat([data_joe,data_donald]).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # flag to be used in the next steps of the project\n",
    "    undone_flag = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6cc2b",
   "metadata": {},
   "source": [
    "# Geolocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a085e5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geolocalization\n",
      "The number of instances in the df is:  1747804\n",
      "The number of instances after dropping the non localized records is:  801011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beltr\\Documents\\GitHub\\DDS_project\\Geographycal_functions.py:71: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: EPSG:4269\n",
      "Right CRS: None\n",
      "\n",
      "  gdf = geopandas.sjoin(usa, gdf, how=\"inner\", op='contains')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets available is: 388853 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "if undone_flag:\n",
    "    print(\"Geolocalization\")\n",
    "    data_all = drop_non_geolocalised(data_all, \"lat\", \"long\")\n",
    "    geo_df = localize_tweets(data_all, \"World Tweet data distribution\")\n",
    "    df = localize_USA(geo_df, \"USA Tweets data distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389ec99",
   "metadata": {},
   "source": [
    "# Preprocessing and text mining\n",
    "We considered only the data localized in the USA.\n",
    "\n",
    "This diminished the number of instances by a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea130fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    # data to drop\n",
    "    to_be_deleted = [\"tweet_id\", \"source\", \"user_id\", \"user_join_date\", \"user_location\", \"continent\", \"collected_at\"]\n",
    "    # drop\n",
    "    df = safe_drop_attr(df, to_be_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    # Text mining\n",
    "    df = text_preprocessing(df)\n",
    "    \n",
    "    # save those preliminary results, since run_time is high\n",
    "    df.to_csv(\".\\Processed_data\\Df_languages_detected.csv\")\n",
    "    \n",
    "    df, lang_state_df = text_mining(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96a8b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.read_csv(\"Df_languages_detected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52744cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_base.copy()\n",
    "df, lang_state_df = text_mining(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "undonw_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    # save the data into a folder\n",
    "    df.to_csv(\".\\Processed_data\\Df_mined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6595b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    # get the lenght of the dataframe to normalize the data\n",
    "    num_instances = len(df[\"STATE_NAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2732c",
   "metadata": {},
   "source": [
    "# Create the ML dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bbfd6",
   "metadata": {},
   "source": [
    "Since we are squeezing all our data into 51 elements, we want to add to each state as much information about the statistical population they represents, we are therefore adding some statistics to the dataframe.\n",
    "Each statistic is related to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a78e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    df.drop(columns=[\"DRAWSEQ\", \"index_right\", ],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    # initialize an empty list where to append the statistics\n",
    "    state_list = []\n",
    "    # group the data by state\n",
    "    df_groupby = df.groupby([\"STATE_NAME\"])\n",
    "    # averages\n",
    "    state_list.append(df_groupby.mean(numeric_only=True).add_suffix(\"_Mean\"))\n",
    "    # skewness\n",
    "    state_list.append(df_groupby.skew(numeric_only=True).add_suffix(\"_Skewness\"))  \n",
    "    # median\n",
    "    state_list.append(df_groupby.median(numeric_only=True).add_suffix(\"_Median\"))  \n",
    "    # count the tweets and normalize the count wrt the total number of instances\n",
    "    state_list.append(df_groupby.count().add_suffix(\"_Counts\") / num_instances)  \n",
    "    # variance\n",
    "    state_list.append(df_groupby.var().add_suffix(\"_Variance\"))\n",
    "    # standard deviation\n",
    "    state_list.append(df_groupby.std().add_suffix(\"_Std\"))\n",
    "    # % of english speakers\n",
    "    state_list.append(lang_state_df)\n",
    "    \n",
    "    \n",
    "    # concatenate all those data to create a large dataframe\n",
    "    state_df = pd.concat(state_list, axis=1)\n",
    "    lang_state_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cdca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undone_flag:\n",
    "    state_df.to_csv(\".\\Processed_data\\States_mined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c348d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_df_scaled = my_scaler.fit_transform(state_df.loc[:, state_df.columns != 'STATE_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_df_scaled = pd.DataFrame(state_df_scaled, columns=state_df.loc[:, state_df.columns != 'STATE_NAME'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_df_scaled = pd.concat([state_df[\"STATE_NAME\"], state_df_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_df_scaled.to_csv(\"State_df_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2dffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
